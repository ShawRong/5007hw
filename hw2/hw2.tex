\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{bm} 
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\newcommand{\R}{\mathbb{R}}
\geometry{a4paper, margin=1in}

\title{MSBD 5007 HW2}
\author{RONG Shuo}
\date{\today}

\begin{document}

\maketitle

\section*{Question1}
Let \(\bm{A} = \begin{bmatrix}
    1 & 2 & 1 & 0 \\ 
    1 & 1 & 2 & 1 \\ 
    1 & 0 & 1 & 2 \\ 
    0 & 1 & 1 & 1  
\end{bmatrix}\) and \(\bm{b} = \begin{bmatrix}
    6 \\
    5 \\
    7 \\
    4 
\end{bmatrix}\). Use the QR factorization to solve the least square problem:
\[
    \min_{\bm{x} \in \R^4} \|\bm{Ax} - \bm{b}\|_2^2.
\]

\subsection*{Answer}
We can calculate the QR factorization of \(\bm{A}\) by Householder transformation. 
\[
    \bm{Q} = \frac{1}{\sqrt{3}}\begin{bmatrix}
       1 & 1 & -1 & 0  \\
       1 & 0 & 1 & -1  \\
       1 & -1 & 0 & 1  \\
       0 & 1 & 1 & 1  
    \end{bmatrix}
\]
\[
    \bm{R} = \begin{bmatrix}
       \sqrt{3} & \sqrt{3} & \frac{4}{\sqrt{3}} & \sqrt{3}  \\
       0 & \sqrt{3} & \frac{1}{\sqrt{3}} & -\frac{1}{\sqrt{3}}  \\
       0 & 0 & \frac{2}{\sqrt{3}} & \frac{2}{\sqrt{3}}  \\
       0 & 0 & 0 & \frac{2}{\sqrt{3}}  
    \end{bmatrix}
\]
\[
    \bm{Q}^T\bm{b}= \frac{1}{\sqrt{3}}\begin{bmatrix}
        1 &1 &1 &0\\
        1 &0 &-1 &1 \\
        -1 &1 &0 &1 \\
        0 &-1 &1 &1 \\
    \end{bmatrix}\begin{bmatrix}
        6 \\
        5 \\
        7 \\
        4
    \end{bmatrix} = \begin{bmatrix}
        6\sqrt{3} \\
        \sqrt{3} \\
        \sqrt{3} \\
        2\sqrt{3}
    \end{bmatrix}
\]


We need to solve \(\bm{R}\bm{x} = \bm{Q}^T\bm{b}\).

\[
    \begin{bmatrix}
       \sqrt{3} & \sqrt{3} & \frac{4}{\sqrt{3}} & \sqrt{3}  \\
       0 & \sqrt{3} & \frac{1}{\sqrt{3}} & -\frac{1}{\sqrt{3}}  \\
       0 & 0 & \frac{2}{\sqrt{3}} & \frac{2}{\sqrt{3}}  \\
       0 & 0 & 0 & \frac{2}{\sqrt{3}}  
    \end{bmatrix} \begin{bmatrix}
        x_1 \\
        x_2 \\
        x_3 \\
        x_4
    \end{bmatrix} = \begin{bmatrix}
        6\sqrt{3} \\
        \sqrt{3} \\
        \sqrt{3} \\
        2\sqrt{3}
    \end{bmatrix}
\]
The solution is :
\[
\begin{bmatrix}
    \frac 5 2 \\
    \frac 5 2 \\
    -\frac 3 2 \\
    3 \\
\end{bmatrix}
\]




\section*{Question2}
Find the best rank-1 approximation with respect to the Frobenius norm to the matrix.
\[\bm{A} = \begin{bmatrix}
   2 & 1 & 1 \\ 
   1 & 2 & 1 \\ 
   1 & 1 & 2 \\ 
\end{bmatrix}\]

\section*{Answer}
\[
\bm{A}\bm{A}^T = \begin{bmatrix}
    2 & 1 & 1 \\
    1 & 2 & 1 \\
    1 & 1 & 2 \\
\end{bmatrix} \begin{bmatrix}
    2 & 1 & 1 \\
    1 & 2 & 1 \\
    1 & 1 & 2 \\
\end{bmatrix} = \begin{bmatrix}
    6 & 5 & 5 \\
    5 & 6 & 5 \\
    5 & 5 & 6 \\
\end{bmatrix}
\]

We need solve the equation, 
\begin{align*}
    -(\lambda - 16)(\lambda - 1)^2 = 0
\end{align*}
The roots are \(\lambda = 16, 1, 1\). 
We can calculate the normalized eigenvectors of \(\lambda = 16\) is \(\frac{1}{\sqrt{3}}\begin{bmatrix}
    1 \\
    1 \\
    1 \\
\end{bmatrix}\),
So the rank-1 approximation is
\begin{align*}
    4 * \frac{1}{\sqrt{3}} \begin{bmatrix}
       1 \\ 
       1 \\ 
       1 \\ 
    \end{bmatrix} \frac{1}{\sqrt{3}} \begin{bmatrix}
        1 & 1 & 1
    \end{bmatrix} = \frac{4}{3} \begin{bmatrix}
        1 & 1 & 1 \\
        1 & 1 & 1 \\
        1 & 1 & 1 \\
    \end{bmatrix}
\end{align*}




\section*{Question3}
Let \(\bm{X}\) and \(\bm{Y}\) be two matrices of dimensions \(m \times n\). Consider the optimization problem \[
    \min_{\bm{Q}\in\R^{m\times n}} \|\bm{X} - \bm{QY}\|_F \text{ subject to } \bm{Q}^T\bm{Q} = \bm{I}_m.
\],
where \(\|\cdot\|_F\) denotes the Frobenius norm and \(I_m\). is the \(m \times n\) identity matrix. This problem aries when we want to align two datasets \(\bm{X} \text{ and }\bm{Y}\) up to an orthogonal linear transformation. Prove that the solution is given by \(\bm{Q} = \bm{U}\bm{V}^T\), where \(\bm{U}\bm{\Sigma}\bm{V}^T\) is the singular value decomposition of \(\bm{XY}^T\), i.e. \(\bm{X}\bm{Y}^T = \bm{U}\bm{\Sigma}\bm{V}^T\).

\section*{Answer}
\begin{align*}
    \|\bm{X} - \bm{Q}\bm{Y}\|_F^2 &= \text{Tr}((\bm{X} - \bm{QY})^T(\bm{X} - \bm{QY})) \\
    &= \text{Tr}(\bm{X}^T\bm{X}) - 2\text{Tr}(\bm{X}^T\bm{Q}\bm{X}) + \text{Tr}(\bm{Y}^T\bm{Q}^T\bm{QY}) \\
    &= \text{Tr}(\bm{X}^T\bm{X}) - 2\text{Tr}(\bm{X}^T\bm{Q}\bm{X}) + \text{Tr}(\bm{Y}^T\bm{Y}) \\
\end{align*}
So the minimization problem is equivalent to
\[
    \max_{\bm{Q}\in\R^{m\times m}} \text{Tr}(\bm{X}^T\bm{Q}\bm{Y}) 
\]
\begin{align*}
    \text{trace}(\bm{Q}\bm{Y}\bm{X}^T) &= \text{trace}(\bm{Q}(\bm{Y}\bm{X}^T)) \\
\end{align*}
Consider the SVD of \(\bm{X}\bm{Y}^T = \bm{U}\bm{\Sigma}\bm{V}^T\).
Let \(\bm{M} = \bm{Y}\bm{X}^T\), and consider the SVD of \(\bm{M} = \bm{V}\bm{\Sigma}\bm{U}^T\).
\begin{align*}
    \text{trace}(\bm{Q}\bm{V}\bm{\Sigma}\bm{U}^T) &= \text{trace}(\bm{U}^T\bm{Q}\bm{V}\bm{\Sigma}) \\
\end{align*}
Let \(\bm{W} = \bm{U}^T\bm{Q}\bm{V}\), and apparently \(\bm{W}\) is an orthogonal matrix.
We know the maximum trace is achieved when \(\bm{W}\) is identity matrix, since for the diagonal entries \( \sigma_{ii} \in \bm{\Sigma}, \sigma >= 0\), and for the element \(w_{ij} \in \bm{W}, w_{ij} <= 1\).
\begin{align*}
    \max \text{trace}(\bm{W}\bm{\Sigma}) = \text{trace}(\bm{I}\bm{\Sigma}) \\
    \bm{W} = \bm{U}^T\bm{Q}\bm{V} = \bm{I} \\
    \bm{Q} = \bm{U}\bm{I}\bm{V}^T = \bm{U}\bm{V}^T 
\end{align*}
In conclusion, the solution is given by \(\bm{Q} = \bm{U}\bm{V}^T\).


\section*{Question4}
Let \(\bm{A} \in \R^{m \times n}\) be a matrix with \(m \geq n\) and full column rank, i.e., \(\text{rank}(\bm{A}) = n\). A \textit{Given rotation} matrix \(\bm{G}_{i, j, \theta} \in \R^{n \times n}\) (for indices \(1 \leq i \leq j \leq n\)) is defined by
\[
    \bm{G}_{i, j, \theta} = \begin{bmatrix}
        \bm{I}_{i-1} &  &  &  &  \\
                     & \cos(\theta) &  & -\sin(\theta) & \\
                     & & \bm{I}_{j - i - 1}& & \\
         & \sin(\theta) && \cos(\theta) &\\
         & &&& \bm{I}_{n-j}
    \end{bmatrix}
\]
where \(\bm{I}_k\) denotes the \(k \times k\) identity matrix, and all unspecified entries are zero. 
Equivalently, \(\bm{G}_{i, j, \theta}\) is identical to the identity matrix except for the four entries in its \textit{i}th and \textit{j}th rows and columns:
\[
    g_{ii} = \text{cos}\theta, g_{ij} = -\text{sin}\theta, g_{ji} = \text{sin}\theta, g_{jj} = \text{cos}\theta.
\] 
Answer the following questions:

(a) Prove that \(\bm{G}_{i,j,\theta}\) is an orthogonal matrix, i.e. show that
\[
    G_{i,j,\theta}^TG_{i,j,\theta} = \bm{I}_n.
\]

(b) Let \(\bm{v} = \begin{bmatrix}
    v_1 \\
    \vdots \\
    v_n \\
\end{bmatrix} \in \R\). Prove that for any indices \(i, j\) with \(1 \leq i < j \leq n\), there exists an angle \(\theta\) such that
\[
    \bm{G}_{i,j,\theta}\bm{v} = \begin{bmatrix}
        v_1 \\
        \vdots \\
        v_{i-1} \\
        \sqrt{v_i^2 + v_j^2} \\
        v_{i+1} \\
        \vdots \\
        v_{j-1} \\
        0 \\
        v_{j+1} \\
        \vdots \\
        v_n
    \end{bmatrix}
\]
In other words, show that the Given rotation can eliminate the \(j\)th entry of \(\bm{v}\) by affecting only the \(i\)th and \(j\)th components.


(c) An \(m \times n\) \(\bm{A}\) is called an \(upper Hessenberg matrix\) if
\[
    a_{ij} = 0 \text{ whenever } i - j > 2.
\]

For \(m \geq n\), the pattern of nonzero entries in \(\bm{A}\) is depicted as
\[
\begin{bmatrix}
    \times & \times & \dots & \times \\
    \times & \times & \dots & \times \\
    0 & \times & \ddots & \vdots \\
    \vdots & \ddots & \ddots & \times \\
    \vdots & \ddots & \ddots & \times \\
    \vdots & \ddots & \ddots & 0 \\
    \vdots & \ddots & \ddots & \vdots \\
    0 & \dots & \dots & 0 \\
\end{bmatrix}
\]

Since \(\bm{A}\) already contains many zeros in its lower triangular portion, the standard Householder QR algorithm can be optimzed.
Develop an algorithm using Givens rotations to compute the QR decomposition of an upper Hessenberg matrix \(\bm{A}\). Your algorithm should achieve a computational cost of \(O(n^2)\) for the reduced (economy-size) QR factorization and \(O(mn)\) for the full QR factorization.


\section*{(a)}
\begin{align*}
    \bm{G}_{i, j, \theta}^T \bm{G}_{i, j, \theta} &=
    \begin{bmatrix}
        \bm{I}_{i-1} &  &  &  &  \\
                     & \cos(\theta) &  & \sin(\theta) & \\
                     & & \bm{I}_{j - i - 1}& & \\
         & -\sin(\theta) && \cos(\theta) &\\
         & &&& \bm{I}_{n-j}
    \end{bmatrix}
    \begin{bmatrix}
        \bm{I}_{i-1} &  &  &  &  \\
                     & \cos(\theta) &  & -\sin(\theta) & \\
                     & & \bm{I}_{j - i - 1}& & \\
         & \sin(\theta) && \cos(\theta) &\\
         & &&& \bm{I}_{n-j}
    \end{bmatrix} \\
    &= 
    \begin{bmatrix}
        \bm{I}_{i-1} &  &  &  &  \\
                     & 1 &  & 0 & \\
                     & & \bm{i}_{j - i - 1}& & \\
         & 0 && 1 &\\
         & &&& \bm{I}_{n-j}
    \end{bmatrix} = \bm{I}_n
\end{align*}

\section*{(b)}
We know,
\begin{align*}
    (\bm{G}_{i,j,\theta} \bm{v})_i = \cos(\theta)v_i - \sin(\theta)v_j
    (\bm{G}_{i,j,\theta} \bm{v})_j = \sin(\theta)v_i + \cos(\theta)v_j
\end{align*}
From 
\[
    (\bm{G}_{i,j,\theta} \bm{v})_i = \sqrt{v_i^2 + v_j^2} 
    (\bm{G}_{i,j,\theta} \bm{v})_j = 0
\]
We can derive that
\begin{align*}
    \cos(\theta) = \frac{v_i}{\sqrt{v_i^2 + v_j^2}} \\
    \sin(\theta) = -\frac{v_j}{\sqrt{v_i^2 + v_j^2}}    
\end{align*}
For any vector \(\bm{v}\), apply \(\bm{G}_{i,j,\theta}\) with \(\theta\) such that:
\begin{align*}
    \cos(\theta) = \frac{v_i}{\sqrt{v_i^2 + v_j^2}} \\
    \sin(\theta) = -\frac{v_j}{\sqrt{v_i^2 + v_j^2}}
\end{align*}
This yields:
\[
    \bm{G}_{i,j,\theta}\bm{v} = \begin{bmatrix}
        v_1 \\
        \vdots \\
        v_{i-1} \\
        \sqrt{v_i^2 + v_j^2} \\
        v_{i+1} \\
        \vdots \\
        v_{j-1} \\
        0 \\
        v_{j+1} \\
        \vdots \\
        v_n
    \end{bmatrix}
\]

\section*{(c)}
We know that for a vector \(\bm{v}_k\), which satisfies \(v_{i} = 0, \text{ if } i > k\) like:
\begin{align*}
    \bm{v}_k = \begin{bmatrix}
        v_0 \\
        \vdots \\
        v_{k - 1}\\
        v_k \\
        0 \\
        \vdots \\
        0 
    \end{bmatrix}
\end{align*}
We can design a matrix \(\bm{G}_{k - 1, k, \theta}\),(where \(\theta\) is calculated by \(v_{k-1} \text{ and } v_k\)) s.t.
\begin{align*}
    \bm{G}_{k - 1, k, \theta} \bm{v}_k = \begin{bmatrix}
        v_0 \\
        \vdots \\
        \sqrt{v_{k - 1}^2 + v_{k}^2}\\
        0 \\
        0 \\
        \vdots \\
        0 
    \end{bmatrix} = \bm{v}_{k-1}' \\
\end{align*}
And \(\|\bm{v}_k\| = \|\bm{v}'_{k - 1}\|\)\\
For vector(\(\bm{w}\)) that follow the pattern of \(\bm{v}_{k - 1}\), we know that:
\begin{align*}
    \bm{G}_{k - 1, k, \theta} \bm{w} = \bm{w}
\end{align*}
s.t.
\begin{align*}
    \bm{G}_{k - 1, k, \theta} \begin{bmatrix}
        \bm{w}_0 & \cdots &\bm{w}_{n - 1} & \bm{v}_{k}
    \end{bmatrix} = \begin{bmatrix}
        \bm{w}_0 & \cdots &\bm{w}_{n - 1} & \bm{v}_{k - 1}'
    \end{bmatrix} 
\end{align*}


We know \(\bm{G}_{i, j, \theta}\) is orthogonal.
According to this, we can do the following steps to get the QR decomposition of an upper Hessenberg matrix \(\bm{A}\):
\begin{align*}
    \bm{G}_{1, 2, \theta_1} \bm{G}_{2, 3, \theta_2} \cdots \bm{G}_{n - 1, n, \theta_{n - 1}}\bm{A}  =  \bm{R} \\
    \bm{A} = \bm{G}_{n - 1, n, \theta_{n - 1}}^T \cdots \bm{G}_{2, 3, \theta_2}^T \bm{G}_{1, 2, \theta_1}^T\bm{R} = \bm{Q}\bm{R}
\end{align*}

Also, it's easy to see:
\begin{align*}
    G_{k - 1, k, \theta}\bm{e}_{i} = \bm{e}_{i}, \forall i \neq k  \\
\end{align*}
And for the multiplication of 2 matrices \(\bm{G}_{k-1, k, \theta}\bm{G}_{k, k+1, \theta}\), we have:
\begin{align*}
    \bm{G}_{k-1, k, \theta}\bm{G}_{k, k+1, \theta} = \bm{G}_{k-1, k, \theta} \begin{bmatrix}
        \bm{e}_0 & \cdots &\bm{e}_{k - 1} &\bm{v}_{k} &\bm{v}_{k+1} &\bm{e}_{k+2} &\cdots & \bm{e}_n \\
    \end{bmatrix}\\
    = \begin{bmatrix}
        \bm{e}_0 & \cdots &\bm{e}_{k - 1} &\bm{v}_{k}'  & \bm{v}_{k+1}'&\bm{e}_{k+2} &\cdots & \bm{e}_n \\
    \end{bmatrix}
\end{align*}
So we know, for any multiplication of any two rotation matrix, we cost \(O(n)\) time to calculate the product.

\begin{algorithm}
\caption{Optimized QR Decomposition of an Upper Hessenberg Matrix using Givens Rotations}
\label{alg:qr_hessenberg}
\begin{algorithmic}[1]
\Require $\bm{A} \in \mathbb{R}^{m \times n}$ (upper Hessenberg matrix)
\Ensure $\bm{Q} \in \mathbb{R}^{m \times m}$ (orthogonal), $\bm{R} \in \mathbb{R}^{m \times n}$ (upper triangular)

\State Initialize $\bm{R} \gets \bm{A}$
\State Initialize $\bm{Q}^T \gets \bm{I}_m$
\For{$j = n$ to $1$}
    \For{$i = j+1$ to $m$}
        \If{$r_{i,j} \neq 0$}
            \State Compute Givens rotation $\bm{G}_{i,j,\theta}$ to zero out $r_{i,j}$
            \State Apply $\bm{G}_{i,j,\theta}$ from the left: $\bm{R} \gets \bm{G}_{i,j,\theta} \bm{R}$
            \State Accumulate $\bm{Q}^T \gets \bm{Q}^T \bm{G}_{i,j,\theta}$
        \EndIf
    \EndFor
\EndFor

\State Compute $\bm{Q} \gets (\bm{Q}^T)^T$
\State \Return $\bm{Q}, \bm{R}$
\end{algorithmic}
\end{algorithm}

Because of the time cost of multiplication like \(\bm{G}_{i,j,\theta}\bm{R}\) and \(\bm{G}_{i,j,\theta} \bm{G}_{m, n, \theta}\) is \(O(n)\), and we need to do this for \(n\) times, so the total time cost is \(O(n^2)\). And the time cost of putting zero if we want full QR factorization is \(O(mn)\).


\end{document}