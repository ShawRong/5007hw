\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{bm} 
\newcommand{\R}{\mathbb{R}}
\geometry{a4paper, margin=1in}

\title{MSBD 5007 HW2}
\author{RONG Shuo}
\date{\today}

\begin{document}

\maketitle

\section*{Question1}
Let \(\bm{A} = \begin{bmatrix}
    1 & 2 & 1 & 0 \\ 
    1 & 1 & 2 & 1 \\ 
    1 & 0 & 1 & 2 \\ 
    0 & 1 & 1 & 1  
\end{bmatrix}\) and \(\bm{b} = \begin{bmatrix}
    6 \\
    5 \\
    7 \\
    4 
\end{bmatrix}\). Use the QR factorization to solve the least square problem:
\[
    \min_{\bm{x} \in \R^4} \|\bm{Ax} - \bm{b}\|_2^2.
\]

\subsection*{(a)}

\subsection*{(b)}

\section*{Question2}
Find the best rank-1 approximation with respect to the Frobenius norm to the matrix.
\[\bm{A} = \begin{bmatrix}
   2 & 1 & 1 \\ 
   1 & 2 & 1 \\ 
   1 & 1 & 2 \\ 
\end{bmatrix}\]
\section*{Question3}
Let \(\bm{X}\) and \(\bm{Y}\) be two matrices of dimensions \(m \times n\). Consider the optimization problem
\[
    \min_{Q\in\R^{m\times n}} \|\bm{X} - \bm{QY}\|_F \text{ subject to } \bm{Q}^T\bm{Q} = \bm{I}_m.
\],
where \(\|\dot\|_F\) denotes the Frobenius norm and \(I_m\). is the \(m \times n\) identity matrix. This problem aries when we want to align two datasets \(\bm{X} \text{ and }\bm{Y}\) up to an orthogonal linear transformation. Prove that the solution is given by \(\bm{Q} = \bm{U}\bm{V}^T\), where \(\bm{U}\bm{\Sigma}\bm{V}^T\) is the singular value decomposition of \(\bm{XY}^T\), i.e. \(\bm{X}\bm{Y}^T = \bm{U}\bm{\Sigma}\bm{V}^T\).

\section*{Question4}
Let \(\bm{A} \in \R^{m \times n}\) be a matrix with \(m \geq n\) and full column rank, i.e., \(\text{rank}(\bm{A}) = n\). A \textit{Given rotation} matrix \(\bm{G}_{i, j, \theta} \in \R^{n \times n}\) (for indices \(1 \leq i \leq j \leq n\)) is defined by
\[
    \bm{G}_{i, j, \theta} = \begin{bmatrix}
        \bm{I}_{i-1} &  &  &  &  \\
                     & \cos(\theta) &  & -\sin(\theta) & \\
                     & & \bm{I}_{j - i - 1}& & \\
         & \sin(\theta) && \cos(\theta) &\\
         & &&& \bm{I}_{n-j}
    \end{bmatrix}
\]
where \(\bm{I}_k\) denotes the \(k \times k\) identity matrix, and all unspecified entries are zero. 
Equivalently, \(\bm{G}_{i, j, \theta}\) is identical to the identity matrix except for the four entries in its \textit{i}th and \textit{j}th rows and columns:
\[
    g_{ii} = cos\theta, g_{ij} = -sin\theta, g_{jj} = cos\theta.
\] 
Answer the following questions:

(a) Prove that \(\bm{G}_{i,j,\theta}\) is an orthogonal matrix, i.e. show that
\[
    G_{i,j,\theta}^TG_{i,j,\theta} = \bm{I}_n.
\]

(b) Let \(\bm{v} = \begin{bmatrix}
    v_1 \\
    \vdots \\
    v_n \\
\end{bmatrix} \in \R\). Prove that for any indices \(i, j\) with \(1 \leq i < j \leq n\), there exists an angle \(\theta\) such that
\[
    \bm{G}_{i,j,\theta}\bm{v} = \begin{bmatrix}
        v_1 \\
        \vdots \\
        v_{i-1} \\
        \sqrt{v_i^2 + v_j^2} \\
        v_{i+1} \\
        \vdots \\
        v_{j-1} \\
        0 \\
        v_{j+1} \\
        \vdots \\
        v_n
    \end{bmatrix}
\]
In other words, show that the Given rotation can eliminate the \(j\)th entry of \(\bm{v}\) by affecting only the \(i\)th and \(j\)th components.


(c) An \(m \times n\) \(\bm{A}\) is called an \(upper Hessenberg matrix\) if
\[
    a_{ij} = 0 \text{ whenever } i - j > 2.
\]

For \(m \geq n\), the pattern of nonzero entries in \(\bm{A}\) is depicted as
\[
\begin{bmatrix}
    \times & \times & \dots & \times \\
    \times & \times & \dots & \times \\
    0 & \times & \ddots & \vdots \\
    \vdots & \ddots & \ddots & \times \\
    \vdots & \ddots & \ddots & \times \\
    \vdots & \ddots & \ddots & 0 \\
    \vdots & \ddots & \ddots & \vdots \\
    0 & \dots & \dots & 0 \\
\end{bmatrix}
\]

Since \(\bm{A}\) already contains many zeros in its lower triangular portion, the standard Householder QR algorithm can be optimzed.
Develop an algorithm using Givens rotations to compute the QR decomposition of an upper Hessenberg matrix \(\bm{A}\). Your algorithm should achieve a computational cost of \(O(n^2)\) for the reduced (economy-size) QR factorization and \(O(mn)\) for the full QR factorization.


\end{document}