\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{bm} 
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\newcommand{\R}{\mathbb{R}}
\geometry{a4paper, margin=1in}

\title{MSBD 5007 HW4}
\author{RONG Shuo}
\date{\today}

\begin{document}

\maketitle

\section*{Question1}
Consider the function \(f: \R^d \to \R\) defined by 
\begin{align*}
    f(\bm{x}) = \sum_{i = 1}^{d}\max(0,1 - x_i),
\end{align*}
where \(x = [x_1, x_2, \cdots, x_n]^T\). Recall that the proximity operator of a function \(g: \R^d \to \R\) is defined as 
\begin{align*}
    \text{prox}_g = \text{arg} \min_{x \in \R^d} \left\{ g(\bm{x}) + \frac{1}{2} \|\bm{x} - \bm{y}\|_2^2\right\}, \bm{y} \in \R^d.
\end{align*}
Derive a closed-form expression for \(\text{prox}_f(\bm{y})\).

\section*{Answer}
Obviously, we can get the \(\text{prox}_f(\bm{y})\) as.
\begin{align*}
    \text{prox}_f(\bm{y}) = \text{arg min}_{\bm{x} \in \R^d} \{ \sum_{i=1}^{d} \max(0, 1 - x_i) + \frac{1}{2} \sum_{i=1}^{d}(x_i - y_i)^2 \}
\end{align*}
We can denote \(\text{prox}_f(\bm{y})_i\) as follow:
\begin{align*}
    \text{prox}_f(\bm{y})_i = \text{arg min}_{x \in \R} \{ \max(0, 1 - x) + \frac{1}{2} (x - y_i)^2 \}
\end{align*}
s.t.
\begin{align*}
    \text{prox}_f(\bm{y}) = \sum_{i=1}^{d} \text{prox}_f(\bm{y})_i
\end{align*}


\noindent
Let \(\phi(x) =  \max(0, 1 - x) + \frac{1}{2} (x - y_i)^2  \) \\
if \(x \geq 1\)
\begin{align*}
    \phi(x) = \{\frac{1}{2} (x - y_i)^2 \} \\
\end{align*}
To minimize this, we need:
\begin{align*}
    &x = y_i &\text{if } y_i \geq 1 \\
    &x = 1        &\text{if } y_i < 1 \\
\end{align*}
Therefore, 
\begin{align*}
    &\text{prox}_f(\bm{y})_i = y_i &\text{if } y_i \geq 1 \\
    &\text{prox}_f(\bm{y})_i = 1 &\text{if } y_i < 1 \\
\end{align*}


if \(x \leq 1\)
\begin{align*}
    \phi(x) = \{ 1 - x + \frac{1}{2} (x - y_i)^2 \} \\
\end{align*}

To minimize this, we need:
\begin{align*}
    &x = y_i + 1 &\text{if }y_i < 0 \\
    &x = 1 &\text{if }y_i \geq 0
\end{align*}
Therefore, 
\begin{align*}
    &\text{prox}_f(\bm{y})_i = y_i + 1 &\text{if } y_i < 0 \\
    &\text{prox}_f(\bm{y})_i = 1 &\text{if } y_i \geq 0 \\
\end{align*}

\noindent
Combining these two, we get:
\begin{align*}
    &\text{prox}_f(\bm{y})_i = \min\{y_i + 1, 1\} = y_i + 1 &\text{if } y_i < 0 \\
    &\text{prox}_f(\bm{y})_i = \min\{1, 1\} = 1 &\text{if } 0 \leq y_i < 1 \\ 
    &\text{prox}_f(\bm{y})_i = y_i &\text{if } y_i \geq 1 \\
\end{align*}

\noindent
So, in conclusion, 
\begin{align*}
    \text{prox}_f(\bm{y}) = [\text{prox}_f(\bm{y})_i]_{i=1}^{d}
\end{align*}
where,
\begin{align*}
    &\text{prox}_f(\bm{y})_i = \begin{cases} y_i + 1 &\text{if } y_i < 0 \\
    1 &\text{if } 0 \leq y_i < 1 \\ 
    y_i &\text{if } y_i \geq 1 \\
    \end{cases}
\end{align*}






\section*{Question2}
In this problem, we study two properties of the 2-norm function \(g(\bm{x}) = \|\bm{x}\|_2\) defined on \(\R^n\). \\
Provide detailed derivations to show that: \\
\noindent
(a) The subdifferential of \(g\) is given by
\begin{align*}
    \partial \|\bm{x}\|_2 = \begin{cases}
        \left\{ \frac{\bm{x}}{\|\bm{x}\|_2}\right\} &\text{if } \bm{x} \ne \bm{0}, \\
        \left\{ \bm{u} \in \R^n | \|\bm{u}\|_2 \leq 1\right\} &\text{if } \bm{x} = \bm{0}.
    \end{cases}
\end{align*}

\noindent
(b) For any \(\alpha > 0\), the proximity operator of \(\alpha \|\cdot\|_2\) is
\begin{align*}
    \text{prox}_{\alpha \|\cdot\|_2}(\bm{y}) = \begin{cases}
        \left(1 - \frac{\alpha}{\|\bm{y}\|_2} \right)\bm{y} &\text{if }\|\bm{y}\|_2 \geq \alpha, \\
        \bm{0} &\text{if } \|\bm{y}\|_2 \leq \alpha.
    \end{cases}
\end{align*}

\section*{Answer}
\subsection*{(a)}
If \(\bm{x} \ne \bm{0}\), we have:
\begin{align*}
    \nabla \|\bm{x}\|_2 = \frac{\bm{x}}{\|\bm{x}\|_2}
\end{align*}
Therefore,
\begin{align*}
    \partial \|\bm{x}\|_2 = \left\{ \frac{\bm{x}}{\|\bm{x}\|_2} \right\}, \text{if } \bm{x} \ne \bm{0}.
\end{align*}

If \(\bm{x} = \bm{0}\), we have:
\begin{align*}
    \|\bm{y}\|_2 &\geq \|\bm{0}\|_2 + \bm{v}^T (\bm{y} - \bm{0}) \\
    \|\bm{y}\|_2 &\geq \bm{v}^T \bm{y}
\end{align*}
According to cs inequality, we know:
\begin{align*}
    &\bm{v}^T \bm{y} \leq \|\bm{v}\|_2\|\bm{y}\|_2 \\
    &\bm{v}^T \bm{y} \leq \|\bm{y}\|_2, \text{  if  } \|\bm{v}\|_2 \leq 1
\end{align*}
Therefore, we get:
\begin{align*}
    \partial \|\bm{x}\|_2 = \left\{ \bm{u} \in \R^n | \|\bm{u}\|_2 \leq 1\right\} &\text{  if  } \bm{x} = \bm{0}.
\end{align*}
In conclusion,
\begin{align*}
    \partial \|\bm{x}\|_2 = \begin{cases}
        \left\{ \frac{\bm{x}}{\|\bm{x}\|_2}\right\} &\text{if } \bm{x} \ne \bm{0}, \\
        \left\{ \bm{u} \in \R^n | \|\bm{u}\|_2 \leq 1\right\} &\text{if } \bm{x} = \bm{0}.
    \end{cases}
\end{align*}


\subsection*{(b)}
By definition, we know:
\begin{align*}
    \text{prox}_{\alpha \|\cdot\|_2} (\bm{y}) = \text{arg} \min_{x \in \R^n} \left\{ \alpha\|\bm{x}\|_2 + \frac{1}{2} \|\bm{x} - \bm{y}\|_2^2\right\}
\end{align*}
We denote \(\phi(\bm{x}) = \alpha\|\bm{x}\|_2 + \frac{1}{2} \|\bm{x} - \bm{y}\|_2^2\), considering the subdifferential of \(\|\bm{x}\|_2\), we have
\begin{align*}
    \partial \phi(\bm{x}) = \begin{cases}
        \alpha \frac{\bm{x}}{\|\bm{x}\|_2} + \bm{x} - \bm{y} &\text{if } \bm{x} \ne \bm{0}, \\
        \alpha\bm{u} - \bm{y}   &\text{if } \bm{x} = \bm{0}, \text{ where }\|\bm{u}\|_2 \leq 1
    \end{cases}
\end{align*}

If \(\bm{x} \ne \bm{0}\), we get the minimizer \(\bm{x}^*\)
\begin{align*}
    &\alpha \frac{\bm{x}^*}{\|\bm{x}^*\|_2} + \bm{x}^* - \bm{y} = \bm{0} \\
    &\bm{y} = \alpha \frac{\bm{x}^*}{\|\bm{x}^*\|_2} + \bm{x}^* \\
\end{align*}
Obviously, \(t\bm{y} = \bm{x}^*, t \ne 0\), therefore,
\begin{align*}
    &\bm{y} = \alpha \frac{\bm{y}}{\|\bm{y}\|_2} + t\bm{y} \\
    &t = 1 - \frac{\alpha}{\|\bm{y}\|_2} \\
    &\bm{x}^* = (1 - \frac{\alpha}{\|\bm{y}\|_2})\bm{y} , \text{ where  } \alpha \ne \|\bm{y}\|_2\\
\end{align*}
So in conclusion, 
\begin{align*}
    &\min \phi(\bm{x}) = \alpha\|\bm{y}\|_2 - \frac{\alpha^2}{2} \text{  if  } \bm{x} \ne \bm{0} \\
    &\bm{x}^* = (1 - \frac{\alpha}{\|\bm{y}\|_2})\bm{y}
\end{align*}
If \(\bm{x} = \bm{0}\), we get
\begin{align*}
    &\phi(\bm{x}) = \frac{\|\bm{y}\|_2^2}{2} \\
    &\min \phi(\bm{x}) = \frac{\|\bm{y}\|_2^2}{2} \\
\end{align*}

By solving the inequality, we know:
\begin{align*}
    &\frac{\|\bm{y}\|_2^2}{2} <  \alpha\|\bm{y}\|_2 - \frac{\alpha^2}{2} \\
    &\|\bm{y}\|_2^2 - 2\alpha\|\bm{y}\|_2 + \alpha^2 < 0
\end{align*}
It holds when \(\alpha > \|\bm{y}\|_2^2\).  \\
Combining these two condition, we know:
\begin{align*}
    \text{prox}_{\alpha \|\cdot\|_2}(\bm{y}) = \begin{cases}
        \left(1 - \frac{\alpha}{\|\bm{y}\|_2} \right)\bm{y} &\text{if }\|\bm{y}\|_2 \geq \alpha, \\
        \bm{0} &\text{if } \|\bm{y}\|_2 \leq \alpha.
    \end{cases}
\end{align*}








\section*{Question 3}
In this problem, we consider the elastic net regression model, which is widely used in statistics for regularized linear regression. The optimization problem is given by
\begin{align*}
    \min_{\bm{x}\in\R^n} \frac{1}{2} \|\bm{A}\bm{x} - \bm{b}\|_2^2 + \lambda_1\|\bm{x}\|_1 + \frac{\lambda_2}{2}\|\bm{x}\|_2^2,
\end{align*}
where \(\bm{A} \in \R^{m \times n}, \bm{b} \in \R^m\), and \(\lambda_1, \lambda_2 > 0\) are regularization parameters. Answer the following: \\
(a) For any \(\beta_1, \beta_2 > 0\), find a closed-form expression for proximity operator \(\text{prox}_{\beta_1\|\cdot\|_1 + \frac{\beta_2}{2}\|\cdot\|_2^2}(\bm{y})\). \\
(b) We apply the forward-backward splitting (i.e. proximal gradient) algorithm. In particular, we apply a forward step for \(\frac{1}{2} \|\bm{A}\bm{x} - \bm{b}\|_2^2\) and a backward step for \(\lambda_1\|\bm{x}\|_1 + \frac{\lambda_2}{2}\|\bm{x}\|_2^2\). Write down the iterative update rule for the resulting algorithm.


\section*{Answer}
\subsection*{(a)}
\begin{align*}
    \text{prox}_{\beta_1\|\cdot\|_1 + \frac{\beta_2}{2}\|\cdot\|_2^2}(\bm{y}) = \text{arg min}_{\bm{x} \in \R^n} \left( \frac{1}{2} \|\bm{x} - \bm{y}\|_2^2 + \beta_1\|\bm{x}\|_1 + \frac{\beta_2}{2} \|\bm{x}\|_2^2\right)
\end{align*}
We can reconstruct the problem as minimizing the following problem:
\begin{align*}
    \phi(x) = \beta_1 |x| + \frac{\beta_2}{2} x^2 + \frac{1}{2} (x - y_i)^2 \\
\end{align*}
And we know, we need
\begin{align*}
    &0 \in \partial \beta_1 |x| + \beta_2 x + x - y_i \\
    &y_i \in \partial \beta_1 |x| + (1 + \beta_2) x \\
\end{align*}
if \(x > 0\),
\begin{align*}
    &y_i \in \beta_1 + (1 + \beta_2) x
\end{align*}
\noindent
Therefore, \(y_i > \beta_1\), \(x = \frac{y_i - \beta_1}{1 + \beta_2}\)

\noindent
if \(x < 0\)
\begin{align*}
    &y_i \in -\beta_1 + (1 + \beta_2) x
\end{align*}
Therefore, \(y_i < -\beta_1\), \(x = \frac{y_i + \beta_1}{1 + \beta_2}\)

\noindent
if \(x = 0\)
\begin{align*}
    y_i \in \beta_1 \partial |x|
\end{align*}
Therefore, \(\beta_2 \leq y_i \leq \beta_1\)



\noindent
Therefore, we get:
\begin{align*}
    x = \begin{cases}
        \frac{y_i - \beta_1}{1 + \beta_2} &\text{if } y_i > \beta_1, \\
        0 &\text{if } |y_i| \leq \beta_1, \\
        \frac{y_i + \beta_1}{1 + \beta_2} &\text{if } y_i < -\beta_1, \\
    \end{cases}
\end{align*}
Therefore:
\begin{align*}
    &\text{prox}_{\beta_1\|\cdot\|_1 + \frac{\beta_2}{2}\|\cdot\|_2^2}(\bm{y})_i = \begin{cases}
        \frac{y_i - \beta_1}{1 + \beta_2} &\text{if } y_i > \beta_1, \\
        0 &\text{if } |y_i| \leq \beta_1, \\
        \frac{y_i + \beta_1}{1 + \beta_2} &\text{if } y_i < -\beta_1, \\
    \end{cases}\\
    &\text{prox}_{\beta_1\|\cdot\|_1 + \frac{\beta_2}{2}\|\cdot\|_2^2}(\bm{y}) = [\text{prox}_{\beta_1\|\cdot\|_1 + \frac{\beta_2}{2}\|\cdot\|_2^2}(\bm{y})_i]_{i = 1}^{n}
\end{align*}


\subsection*{(b)}
\begin{align*}
    &\bm{x}^{(k+1)} = T_{\alpha_k \lambda_1 \lambda_2} \left(x^{(k)} - \alpha_k \bm{A}^T(\bm{A}\bm{x}^{(k)} - \bm{b}) \right) \\
    &T_{\alpha_k \lambda_1 \lambda_2}(\bm{x}) = \text{prox}_{\alpha_k\lambda_1\|\cdot\|_1 + \frac{\alpha_k\lambda_2}{2}\|\cdot\|_2^2}(\bm{x})
\end{align*}



\section*{Question 4}
Let \(g: \R^n \to \R\) be a convex function. Prove the following properties: \\
(a) For any \(\bm{x}, \bm{y} \in \R^n\) and for any \(\bm{u} \in \partial g(\bm{x})\) and \(\bm{v} \in \partial g(\bm{y})\), show that
\begin{align*}
    \langle \bm{u} - \bm{v}, \bm{x} - \bm{y}\rangle \geq 0.
\end{align*}
\textit{Hint: Use the definition of the subdifferential}

\noindent
(b) Prove that the proximity operator of \(g\) is nonexpansive; that is, for all \(\bm{x}, \bm{y} \in \R^n\),
\begin{align*}
    \|\text{prox}_g (\bm{x}) - \text{prox}_g(\bm{y})\|_2 \leq \|\bm{x} - \bm{y}\|_2
\end{align*}
\textit{Hint: Apply the result from part (a)}

\noindent
(c) Show that a point \(\bm{x}^*\) minimizes \(g\) if and only if
\begin{align*}
    \bm{x}^* = \text{prox}_g(\bm{x}^*)
\end{align*}

\section*{Answer}
\subsection*{(a)}
We know \(\forall \bm{z} \in \R^n\), we get
\begin{align*}
    &g(\bm{z}) \geq g(\bm{x}) + \langle \bm{u}, \bm{z} - \bm{x}\rangle \\
    &g(\bm{z}) \geq g(\bm{y}) + \langle \bm{v}, \bm{z} - \bm{y}\rangle \\
\end{align*}
So we choose \(\bm{z} = \bm{y}, \bm{z} = \bm{x}\) separately.
\begin{align*}
    &g(\bm{y}) \geq g(\bm{x}) + \langle \bm{u}, \bm{y} - \bm{x}\rangle \\
    &g(\bm{x}) \geq g(\bm{y}) + \langle \bm{v}, \bm{x} - \bm{y}\rangle \\
    &g(\bm{y}) + g(\bm{x}) \geq g(\bm{x}) + g(\bm{y}) + \langle \bm{u}, \bm{y} - \bm{x}\rangle + \langle \bm{v}, \bm{x} - \bm{y}\rangle \\
    &0 \geq \langle \bm{u} - \bm{v}, \bm{y} - \bm{x}\rangle \\
    &\langle \bm{u} - \bm{v}, \bm{x} - \bm{y}\rangle \geq 0
\end{align*}

\subsection*{(b)}
For \(\text{prox}_g(\bm{y}) = \text{arg min}_{\bm{x} \in \R^n} \phi (\bm{x})\). Let's denote \(\phi (\bm{x}) = g(\bm{x}) + \frac{1}{2}\|\bm{x} - \bm{y}\|_2^2\), \(\partial \phi (\bm{x}) = \partial g(\bm{x}) + \bm{x} - \bm{y}\) \\
The minimizer \(\bm{x}^*\) of \(\phi(\bm{x})\) satisfies \(\bm{0} \in \partial g(\bm{x}^*) + \bm{x}^* - \bm{y}\), and \(\text{porx}_g(\bm{y}) = \bm{x}^*\) \\
Suppose \(\text{prox}_g(\bm{x}) = \bm{p}, \text{prox}_g(\bm{x}) = \bm{q}\)
\begin{align*}
    &\bm{0} \in \partial g(\bm{p}) + (\bm{p} - \bm{x}) \\
    &\bm{x} - \bm{p} \in \partial g(\bm{p}) \\
    &\bm{0} \in \partial g(\bm{q}) + (\bm{q} - \bm{y}) \\
    &\bm{y} - \bm{q} \in \partial g(\bm{q}) \\
\end{align*}
by (a)
\begin{align*}
    &\langle (\bm{x} - \bm{p}) - (\bm{y} - \bm{q}), \bm{p} - \bm{q} \rangle \geq 0 \\
    &\langle (\bm{x} - \bm{y}) - (\bm{p} - \bm{q}), \bm{p} - \bm{q} \rangle \geq 0 \\
    &\langle \bm{x} - \bm{y}, \bm{p} - \bm{q} \rangle \geq \| \bm{p} - \bm{q} \|_2^2\\
\end{align*}
By cs inequality, we know:
\begin{align*}
    \langle \bm{x} - \bm{y}, \bm{p} - \bm{q} \rangle \leq \|\bm{x} - \bm{y}\|_2\|\bm{p} - \bm{q}\|_2
\end{align*}
Thus,
\begin{align*}
    & \|\bm{x} - \bm{y}\|_2 \geq \| \bm{p} - \bm{q} \|_2 \\
    &\|\text{prox}_g (\bm{x}) - \text{prox}_g(\bm{y})\|_2 \leq \|\bm{x} - \bm{y}\|_2
\end{align*}

\subsection*{(c)}
if \(\bm{x}^*\) is a minimizer of g, we get
\begin{align*}
    \bm{0} \in \partial g(\bm{x}^*)
\end{align*}
We know
\begin{align*}
    \bm{0} \in \partial g(\bm{z}) + \bm{z} - \bm{x^*} \implies \text{prox}_g(\bm{x}^*) = \bm{z}
\end{align*}
And we know:
\begin{align*}
    &\bm{0} \in \partial g(\bm{x}^*) \\
    &\bm{0} \in \partial g(\bm{x}^*) + \bm{x}^* - \bm{x}^*
\end{align*}
So we can conclude
\begin{align*}
    \text{prox}_g(\bm{x}^*) = \bm{\bm{x}^*}
\end{align*}

\noindent
if \(\bm{x}^* = \text{prox}_g(\bm{x}^*)\)
\begin{align*}
    &\bm{0} \in \partial g(\bm{\bm{x}^*}) + \bm{\bm{x}^*} - \bm{x^*} \\
    &\bm{0} \in \partial g(\bm{\bm{x}^*}) \\
\end{align*}
We know
\begin{align*}
    \text{g is a convex function and } \bm{0} \in \partial g(\bm{x})  \implies \bm{x} \text{ is a minimizer of g.}
\end{align*}

So in conclusion, 
\begin{align*}
     \bm{x}^* \text{ minimizes } g \iff \bm{x}^* = \text{prox}_g(\bm{x}^*)
\end{align*}







\end{document}