\documentclass[12pt]{article}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[margin=2.5cm]{geometry}
\usepackage{bm} 
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\newcommand{\R}{\mathbb{R}}

\title{MSBD5007 Optimization and Matrix Computation\\
Homework 5}
\author{RONG Shuo}

\begin{document}

\maketitle

\begin{enumerate}
\item Find explicit formulas for the projection of $y \in \mathbb{R}^n$ onto the following non-empty, closed, and convex sets $S \subset \mathbb{R}^n$, respectively.
    \begin{enumerate}
    \item The unit $\infty$-norm ball
    \[S = \{x \in \mathbb{R}^n \mid \|x\|_\infty \leq 1\}.\]
    \item The closed halfspace
    \[S = \{x \in \mathbb{R}^n \mid a^T x \leq b\},\]
    where $a \in \mathbb{R}^n$, $a \neq 0$, and $b \in \mathbb{R}$ are given.
    \end{enumerate}

\item Consider the optimization problem in $x = (x_1, x_2, x_3) \in \mathbb{R}^3$:
\[\begin{aligned}
\min_{x\in\mathbb{R}^3} &\frac{1}{2}(x_1^2 + 4x_2^2 + 9x_3^2) - (4x_1 + 2x_2),\\
\text{s.t.} \quad &x_1 + x_2 + x_3 = 3,\\
&x_i \geq 0, \quad i = 1, 2, 3.
\end{aligned}\]
    \begin{enumerate}
    \item Write down the KKT conditions (stationarity, feasibility, complementary slackness).
    \item Solve the KKT system to find the optimal solution $x^*$, the Lagrange multiplier $\lambda^*$ for the equality constraint, and the multipliers $\mu^* = (\mu_1, \mu_2, \mu_3)$ for the inequality constraints.
    \end{enumerate}

\item We wish to compute the projection of $y \in \mathbb{R}^n$ onto the unit $\ell_1$ball, i.e. solve
\[\begin{aligned}
\min_{x\in\mathbb{R}^n} &\frac{1}{2}\|x - y\|_2^2\\
\text{s.t.} \quad &\|x\|_1 \leq 1.
\end{aligned}\]
    \begin{enumerate}
    \item Derive the Lagrange dual problem and show it can be written as
    \[\max_{\lambda\geq 0} d(\lambda), \quad d(\lambda) = \sum_{i=1}^n h_\lambda(y_i) - \lambda,\]
    where $h_\lambda : \mathbb{R} \to \mathbb{R}$ is the so-called Huber's function (which is a smooth function consisting of a quadratic and two linear pieces) defined by
    \[h_\lambda(t) = \begin{cases}
    \frac{1}{2}t^2, & |t| \leq \lambda,\\
    \lambda |t| - \frac{1}{2}\lambda^2, & |t| \geq \lambda.
    \end{cases}\]
    \item Prove that strong duality holds.
    \item Find the optimal dual multiplier $\lambda^*$.
    \item Give an expression of the projection in terms of $\lambda^*$.
    \end{enumerate}
\item Let $S \subseteq \mathbb{R}^n$ be a nonempty, closed, convex set, and let $\|\cdot\|_2$ denote 2-norm. The projection of any point $y \in \mathbb{R}^n$ onto $S$ is defined by
\[\mathcal{P}_S(y) = \arg\min_{x\in S} \|x-y\|_2.\]
Prove that the projection $\mathcal{P}_S : \mathbb{R}^n \to S$ is nonexpansive: for all $x, y \in \mathbb{R}^n$,
\[\|\mathcal{P}_S(x) - \mathcal{P}_S(y)\|_2 \leq \|x-y\|_2.\]
\end{enumerate}

\section{Answer}
\subsection{(1)}
\subsubsection{(a)}
We know the projection:
\begin{align*}
    \mathcal{P}_S(y) = \arg\min_{\bm{x} \in S} \|\bm{x} - \bm{y}\|_2.
\end{align*}
We consider

\[\min_{\bm{x}\in\mathbb{S}} \|\bm{x}-\bm{y}\|_2\]

\noindent
This means:

\[\min_{\bm{x} \in \mathbb{S}} \sum_{i=1}^n (x_i - y_i)^2\]

\noindent
We can solve for each component $x_i$ independently:

\[\min_{x_i \in [-1,1]} (x_i - y_i)^2\]

\noindent
This is just the Euclidean projection of a scalar onto the interval $[-1,1]$, which gives:

\[x_i = \min(\max(y_i, -1), 1)\]

\noindent
and \[\bm{x} = [x_i]_{1}^{n}\]

\subsubsection{(b)}

We know the projection:
\begin{align*}
    \mathcal{P}_S(y) = \arg\min_{\bm{x} \in S} \|\bm{x} - \bm{y}\|_2 
\end{align*}

\begin{align*}
    \mathcal{L}(\bm{x}, \lambda) = \frac{1}{2} \|\bm{x} - \bm{y}\|_2^2 + \lambda (\bm{a}^T \bm{x} - b), \quad \lambda \geq 0
\end{align*}

We know the KKT conditions:
\begin{align*}
    \nabla_x \mathcal{L} &= \bm{x} - \bm{y} + \lambda \bm{a} = 0 \\
    \bm{x}^* &= \bm{y} - \lambda \bm{a}
\end{align*}

and
\begin{align*}
    \bm{a}^T \bm{x}^* \leq b \\
    \bm{a}^T (\bm{y} - \lambda \bm{a}) \leq b \\
    \bm{a}^T \bm{y} - \lambda \|\bm{a}\|^2 \leq b \\
    \lambda \geq \frac{\bm{a}^T\bm{y} -b}{\|\bm{a}\|^2}
\end{align*}

and we know:
\begin{align*}
    \lambda(\bm{a}^T\bm{x} - b) = 0 \\
    \lambda(\bm{a}^T\bm{y} - \lambda\|\bm{a}\|^2 - b) = 0 \\
\end{align*}
So we get \(\lambda = \max(0, \frac{\bm{a}^T\bm{y} - b}{\|\bm{a}\|^2})\)

\noindent
So we can get 
\begin{align*}
    \mathcal{P}_S(\bm{y}) = \bm{y} - \max(0, \frac{\bm{a}^T\bm{y} - b}{\|\bm{a}\|^2})\bm{a}
\end{align*}

\subsection{(2)}
\subsubsection{(a)}
We can write the Lagrangian Function:
\begin{align*}
    \mathcal{L}(x, \lambda, \mu) = \frac{1}{2} (x_1^2 + 4x_2^2 + 9x_3^2) - (4x_1 + 2x_2) - \lambda(x_1 + x_2 + x_3 - 3) - \sum_{i=1}^{3} \mu_i x_i 
\end{align*}

\begin{align*}
    \nabla_{x} \mathcal{L} = 0 \\
    x_1 - 4 - \lambda - \mu_1 = 0 \\
    4x_2 - 2 - \lambda - \mu_2 = 0 \\
    9x_3 - \lambda - \mu_3 = 0 \\
\end{align*}

\begin{align*}
    x_1 + x_2 + x_3 = 3 \\
    x_i \geq 0
\end{align*}

\begin{align*}
    \mu_i \geq 0 \\
    \mu_i x_i = 0
\end{align*}

\subsubsection{(b)}
We need to do analysis of the solution first.

\begin{align*}
    x_1 > 0, x_2 > 0, x_3 > 0 \quad \text{invalid} \\
    x_1 = 0, x_2 > 0, x_3 > 0 \quad \text{invalid} \\
    x_1 > 0, x_2 = 0, x_3 > 0 \quad \text{invalid} \\
    x_1 > 0, x_2 > 0, x_3 = 0 \quad \text{valid} \\
\end{align*}

if \(x_3 = 0\), we know \(\mu_1 = 0, \mu_2 = 0\). Therefore, we get:
\begin{align*}
    x_1 = \lambda + 4 \\
    x_2 = \frac {\lambda + 2} {4} \\
    \mu_3 = -\lambda
\end{align*}
We know \(x_1 + x_2 = 3\).
\begin{align*}
    \lambda + 4 + \frac {\lambda + 2} {4} = 3 \\
    \lambda = -\frac{6}{5}
\end{align*}
s.t.
\begin{align*}
    x_1 = \frac{14}{5} \\
    x_2 = \frac{1}{5} \\
    x_1 = 0 \\
    \mu_3 = \frac{6}{5} \\
\end{align*}

Therefore, we can conclude:
\begin{align*}
    x_1 = \frac{14}{5} \\
    x_2 = \frac{1}{5} \\
    x_1 = 0 \\
    \mu_1 = 0 \\
    \mu_2 = 0 \\
    \mu_3 = \frac{6}{5} \\
    \lambda = -\frac{6}{5}
\end{align*}

\subsection{(3)}
\subsubsection{(a)}
We know the Lagrange formula:
\begin{align*}
    \mathcal{L}(\bm{x}, \lambda) = \frac{1}{2} \|\bm{x} - \bm{y}\|_2^2 + \lambda(\|\bm{x}\|_1 - 1)
\end{align*}

And the primal problem can be written as:
\begin{align*}
    \min_{\bm{x}} \max_{\lambda \geq 0} \mathcal{L}(\bm{x}, \lambda)
\end{align*}
And the dual problem can be written as:
\begin{align*}
    \max_{\lambda \geq 0} \min_{\bm{x}} \mathcal{L}(\bm{x}, \lambda) \\
    \max_{\lambda \geq 0} \min_{\bm{x}} \frac{1}{2} \|\bm{x} - \bm{y}\|_2^2 + \lambda(\|\bm{x}\|_1 - 1)
\end{align*}
The problem is separable in x, so we can reformulate this problem as:
\begin{align*}
    \max_{\lambda \geq 0} \min_{x_i} \frac{1}{2} (x_i - y_i)^2 + \lambda|x_i| - \lambda
\end{align*}
Consider \(\min_{x_i} \frac{1}{2} (x_i - y_i)^2 + \lambda|x_i|\).
Apparently, this is a soft-thresholding operator which have been solved in hw4. We can derive:
\begin{align*}
    x_i^* = \begin{cases}
        y_i - \lambda &\text{if}\quad y_i > \lambda \\
        y_i + \lambda &\text{if}\quad y_i < -\lambda \\
        0             &\text{if}\quad |y_i| \leq \lambda
    \end{cases}
\end{align*}

If \(|y_i| \leq \lambda\):
\begin{align*}
    \min_{x_i} \frac{1}{2} (x_i - y_i)^2 + \lambda|x_i| = \frac{1}{2} y_i^2 \\
\end{align*}

If \(y_i > \lambda\):
\begin{align*}
    \min_{x_i} \frac{1}{2} (x_i - y_i)^2 + \lambda|x_i|&\\
    &= \frac{1}{2} \lambda^2 + \lambda y_i - \lambda^2  \\
    &= \lambda y_i - \frac{1}{2} \lambda^2\\
\end{align*}


If \(y_i < \lambda\):
\begin{align*}
    \min_{x_i} \frac{1}{2} (x_i - y_i)^2 + \lambda|x_i|&\\
    &= \frac{1}{2} \lambda^2 - \lambda y_i - \lambda^2  \\
    &= -\lambda y_i - \frac{1}{2} \lambda^2\\
\end{align*}

In conclusion,
\begin{align*}
    h_{\lambda}(y_i) = \begin{cases}
        \frac{1}{2} y_i^2 &\text{if} \quad |y_i| \leq \lambda \\
        \lambda|y_i| - \frac{1}{2} \lambda^2 &\text{if} \quad |y_i| > \lambda \\
    \end{cases}
\end{align*}
And the dual problem can be written as:
\[\max_{\lambda\geq 0} d(\lambda), \quad d(\lambda) = \sum_{i=1}^n h_\lambda(y_i) - \lambda,\]


\subsubsection{(b)}
The objective \(\frac{1}{2} \|x - y\|_2^2\) and the constraint \(\|x\|_1 \leq 1\) is convex. \\
And we know for Slater's condition, we can easily find \(x = 0, x \in \R^n\), s.t. \(\|x\|_1 < 1\). \\
Hence, strong duality holds.

\subsubsection{(c)}
\[d(\lambda) = \sum_{i=1}^n h_\lambda(y_i) - \lambda,\]
\[d'(\lambda) = \sum_{i=1}^{n} h_\lambda'(y_i) - 1\]

\begin{align*}
    h_{\lambda}(y_i) = \begin{cases}
        \frac{1}{2} y_i^2 &\text{if} \quad |y_i| \leq \lambda \\
        \lambda|y_i| - \frac{1}{2} \lambda^2 &\text{if} \quad |y_i| > \lambda \\
    \end{cases}
\end{align*}

\begin{align*}
    \partial h'_{\lambda}(y_i) = \begin{cases}
        0 &\text{if} \quad |y_i| \leq \lambda \\
        |y_i| - \lambda  &\text{if} \quad |y_i| > \lambda \\
    \end{cases}
\end{align*}

\begin{align*}
    d'(\lambda) = \sum_{i=1}^{n} h_\lambda'(y_i) - 1 \\
    d'(\lambda) = \sum_{i=1}^{n} \max\{|y_i| - \lambda, 0\} - 1 \\
\end{align*}
Hence
\begin{align*}
    \lambda^* = \frac{\sum_{i=n-k+1}^{n}|y_{i}| - 1}{k}
\end{align*}
where \(|y_i|\) is rearrange by descending order, and k is the number of \(|y_i|\) values greater than \(\lambda\).

\subsubsection{(d)}

\begin{align*}
    x_i^* = \begin{cases}
        y_i - \lambda^* &\text{if}\quad y_i > \lambda \\
        y_i + \lambda^* &\text{if}\quad y_i < -\lambda \\
        0             &\text{if}\quad |y_i| \leq \lambda
    \end{cases}
\end{align*}

\begin{align*}
    \lambda^* = \frac{\sum_{i=n-k+1}^{n}|y_{i}| - 1}{k}
\end{align*}
where \(|y_i|\) is rearrange by descending order, and k is the number of \(|y_i|\) values greater than \(\lambda\).


\subsection{(4)}
\begin{align*}
    \|\mathcal{P}_S(\bm{x}) - \mathcal{P}_S(\bm{y}) \|_2 &\leq \|\bm{x} - \bm{y}\|_2 \\
    \|\text{arg} \min_{\bm{z} \in S} \|\bm{z} - \bm{x}\|_2 - \text{arg} \min_{\bm{z} \in S} \|\bm{z} - \bm{x}\|_2 \|_2 &\leq \|\bm{x} - \bm{y}\|_2
\end{align*}

\begin{align*}
    \langle \bm{y} - \mathcal{P}_S \bm{y}, \bm{z} - \mathcal{P}_S \bm{y}\rangle \leq 0, \forall \bm{z} \in S \\
    \langle \bm{x} - \mathcal{P}_S \bm{x}, \bm{z} - \mathcal{P}_S \bm{x}\rangle \leq 0, \forall \bm{z} \in S
\end{align*}
Substitute \(z\) as \(\mathcal{P}_S x\) and \(\mathcal{P}_S y\) independently, and add the inequality up.
\begin{align*}
    \langle \bm{y} - \mathcal{P}_S \bm{y}, \mathcal{P}_S \bm{x} - \mathcal{P}_S \bm{y}\rangle \leq 0, \forall \bm{z} \in S \\
    \langle \bm{x} - \mathcal{P}_S \bm{x}, \mathcal{P}_S \bm{y} - \mathcal{P}_S \bm{x}\rangle \leq 0, \forall \bm{z} \in S \\
    \langle \bm{y} - \mathcal{P}_S \bm{y}, \mathcal{P}_S \bm{x} - \mathcal{P}_S \bm{y}\rangle + \langle \bm{x} - \mathcal{P}_S \bm{x}, \mathcal{P}_S \bm{y} - \mathcal{P}_S \bm{x}\rangle \leq 0 \\
    \langle \bm{y} - \mathcal{P}_S \bm{y} - \bm{x} + \mathcal{P}_S \bm{x}, \mathcal{P}_S \bm{x} - \mathcal{P}_S \bm{y}\rangle  \leq 0 \\
    \langle \bm{y} - \bm{x} - (\mathcal{P}_S \bm{y}  - \mathcal{P}_S \bm{x}), \mathcal{P}_S \bm{x} - \mathcal{P}_S \bm{y}\rangle  \leq 0 \\
    \langle \bm{x} - \bm{y}, \mathcal{P}_S \bm{x} - \mathcal{P}_S \bm{y}\rangle  \geq \|\mathcal{P}_S \bm{x} - \mathcal{P}_S \bm{y}\|_2^2 \\
\end{align*}
By CS-inequality,
\begin{align*}
    \langle \bm{x} - \bm{y}, \mathcal{P}_S \bm{x} - \mathcal{P}_S \bm{y}\rangle  \leq \|\bm{x} - \bm{y}\|_2 \|\mathcal{P}_S \bm{x} - \mathcal{P}_S \bm{y}\|_2 \\
\end{align*}
\begin{align*}
    \|\bm{x} - \bm{y}\|_2 \|\mathcal{P}_S \bm{x} - \mathcal{P}_S \bm{y}\|_2 \geq \|\mathcal{P}_S \bm{x} - \mathcal{P}_S \bm{y}\|_2^2 \\
    \|\bm{x} - \bm{y}\|_2 \geq \|\mathcal{P}_S \bm{x} - \mathcal{P}_S \bm{y}\|_2 \\
    \|\bm{x} - \bm{y}\|_2 \geq \|\mathcal{P}_S \bm{x} - \mathcal{P}_S \bm{y}\|_2 \\
\end{align*}
\end{document}