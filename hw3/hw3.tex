\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{bm} 
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\newcommand{\R}{\mathbb{R}}
\geometry{a4paper, margin=1in}

\title{MSBD 5007 HW2}
\author{RONG Shuo}
\date{\today}

\begin{document}

\maketitle

\section*{Question1}
Determine the convexity of the following functions, where \(\bm{x} \in \R^n \) and \(\bm{X} \in \mathbb{S}_{++}^{n}\)(the set of symmetric positive definite matrices). Justify your answer.

(a) \(f(\bm{x}) = \text{log}(e^{x_1} + e^{x_2} + \cdots + e^{x_n})\).

(b) \(f(\bm{X}) = \text{log} \text{det}(\bm{X})\).

\section*{Answer}
\subsection*{(a)}
\begin{align*}
    \nabla f(\bm{x}) = (\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \cdots, \frac{\partial f}{\partial x_n})
\end{align*}
\begin{align*}
    \frac{\partial f}{\partial x_i} &= \frac{1}{e^{x_1} + e^{x_1} + \cdots + e^{x_1}} \times \frac{\partial }{\partial x_i} (e^{x_1} + e^{x_2} + \cdots + e^{x_n}) \\
    &=  \frac{e^{x_i}}{e^{x_1} + e^{x_2} + \cdots + e^{x_n}} 
\end{align*}
Therefore,
\begin{align*}
    \nabla f(\bm{x}) = (\frac{e^{x_1}}{\sum_{i = 1}^{n} e^{x_i}}, \frac{e^{x_2}}{\sum_{i = 1}^{n} e^{x_i}}, \cdots ,\frac{e^{x_1}}{\sum_{i = n}^{n} e^{x_i}})
\end{align*}
We know,
\begin{align*}
    \bm{H}_{ij} &= \frac{\partial^2 f}{\partial x_i \partial y_i} \\
    \bm{H}_{ii} &= \frac{}{\partial x_i} (\frac{e^{x_i}}{S}) = \frac{e^{x_i} (S - e^{x_i})}{S^2} \text{    if \(i = j\) } \\ 
    \bm{H}_{ij} &= -\frac{e^{x_i}e^{x_j}}{S^2} \text{    if \(i \neq j\) } \\
    \nabla^2 f(\bm{x}) &= \frac{1}{S}\text{diag}(\bm{e}) - \frac{1}{S^2}\bm{e}\bm{e}^T
\end{align*}
where \( \bm{e} = (e^{x_1},  e^{x_2}, \cdots, e^{x_n} )^T \), and \(S = \sum_{i = 1}^{n} e^{x_i}\)

\begin{align*}
    \bm{z}\nabla^2 f(\bm{x})\bm{z}^T &= \frac{1}{S}\sum_{i=1}^{n}e^{x_i}z_i^2 - \frac{1}{S^2}(\sum_{i=1}^{n}e^{x_i}z_i)^2 \\
    (\sum_{i=1}^{n}e^{x_i}z_i)^2 &\leq (\sum_{i=1}^{n}e^{x_i}z_i^2)(\sum_{i=1}^{n}e^{x_i}) \\
    \frac{1}{S}\sum_{i=1}^{n}e^{x_i}z_i^2 &\geq \frac{1}{S^2}(\sum_{i=1}^{n}e^{x_i}z_i)^2 \\
    \bm{z}\nabla^2 f(\bm{x})\bm{z}^T &\geq 0
\end{align*}
Thus, we prove that Hessian is positive semi-definite, so we can conclude \(f(\bm{x})\) is convex.

\subsection*{(b)}
Let \(\bm{X} \in \mathbb{S}_{++}^{n}\) and \(\bm{V} \in \mathbb{S}^n\), we define \(g(t) = \text{log} \text{det} (\bm{X} + t\bm{V})\), where \(\bm{X} + t\bm{V}\) is symmetric, positive and definite.
\begin{align*}
    g(t) &= \text{log} \text{det} (\bm{X} + t\bm{V}) \\
    &= \text{log} \text{det} (\bm{X}^{\frac{1}{2}}(\bm{I} + t\bm{X}^{-\frac{1}{2}} \bm{V} \bm{X}^{-\frac{1}{2}})\bm{X}^{\frac{1}{2}}) \\
    &= \text{log} \text{det} \bm{X} + \text{log} \text{det} (\bm{I} + t\bm{\Lambda}) \\
    &= \text{log} \text{det} \bm{X} + \sum_{i=1}^{n} \text{log}(1 + t\lambda_i)
\end{align*}
where \(\lambda_i\) is the eigenvalues of \(\Lambda\)
\begin{align*}
    g''(t) = \sum_{i=1}^{n} \frac{-\lambda_i^2}{(1 + t \lambda_i)^2} \leq 0
\end{align*}
Thus \(g''(t) \leq 0\) for all \(t\) where \(\bm{X} + t\bm{V}\) is symmetric, positive and definite, so \(g(t)\) is concave.
Since \(g(t)\) is concave for any direction, \(f(X) = \text{log} \text{det} \bm{X}\) is concave on \(\mathbb{S}_{++}^{n}\).





\section*{Question2}
Consider the linear system \(\bm{Ax} = \bm{b}\), with \(\bm{A} = \begin{bmatrix}
    2 & 1\\
    1 & 4
\end{bmatrix}\) and \(\bm{b} \in \begin{bmatrix}
    2 \\
    -4
\end{bmatrix}\), and the initial guess \(\bm{x}_0 = \begin{bmatrix}
    0 \\
    0
\end{bmatrix}\).

(a) Present the first two update iterations using the \textbf{steepest descent algorithm}. 

(b) Present the first two updated iterations using the \textbf{conjugate gradient algorithm}.


\section*{Answer}
\subsection*{(a)}
We know if we want use steepest descent algorithm to solve the linear system, we need:
\begin{align*}
    \alpha_k = \frac{\bm{r}_k^T\bm{r}_k}{\bm{r}_k^T\bm{A}\bm{r}_k}
\end{align*}
where \(\bm{r}_{k+1} = \bm{r}_k - \alpha_k \bm{A} \bm{r}_k\)

\begin{align*}
    \bm{r}_0 &= \bm{b} - \bm{A}\bm{x}_0 = \begin{bmatrix}
        2\\
        -4
    \end{bmatrix} \\
    \alpha_0 &= \frac{5}{14} \\
    \bm{x}_1 &= \begin{bmatrix}
        0\\
        0
    \end{bmatrix} + \frac{5}{14}\begin{bmatrix}
        2 \\
        4
    \end{bmatrix} = \begin{bmatrix}
        \frac{5}{7} \\
        -\frac{10}{7}
    \end{bmatrix}\\
    \bm{r}_1 &= \begin{bmatrix}
        0\\
        0
    \end{bmatrix} - \frac{5}{14}\begin{bmatrix}
        2 \\
        4
    \end{bmatrix} = \begin{bmatrix}
        2\\
        1
    \end{bmatrix}
\end{align*}
next iteration:
\begin{align*}
    \alpha_1 &= \frac{5}{16} \\
    \bm{x}_2 &= \begin{bmatrix}
        \frac{5}{7} \\
        -\frac{10}{7}
    \end{bmatrix} + \frac{5}{16} \begin{bmatrix}
        2 \\
        1
    \end{bmatrix} = \begin{bmatrix}
        \frac{75}{56} \\
        -\frac{125}{112}
    \end{bmatrix} \\
    \bm{r}_2 &= \begin{bmatrix}
        2 \\
        1
    \end{bmatrix} - \frac{5}{16}\begin{bmatrix}
        5 \\
        6
    \end{bmatrix} = \begin{bmatrix}
        \frac{7}{16} \\
        -\frac{14}{16}
    \end{bmatrix}
\end{align*}
In conclusion, \(\bm{x}_1 = \begin{bmatrix}
    \frac{5}{7} \\
    -\frac{10}{7}
\end{bmatrix} \bm{x}_2 = \begin{bmatrix}
    \frac{75}{56} \\
    -\frac{125}{112}
\end{bmatrix}\)

\subsection*{(b)}
\begin{align*}
    \bm{r}_0 &= \bm{b} - \bm{A}\bm{x}_0 = \begin{bmatrix}
        2 \\
        -4
    \end{bmatrix} \\
    \alpha_0 &= \frac{\bm{r}_0^T\bm{r}_0}{\bm{r}_0^T \bm{A} \bm{r}_0} = \frac{5}{14} \\ 
    \bm{x}_1 &= \begin{bmatrix}
        \frac{5}{7} \\
        -\frac{10}{7}
    \end{bmatrix}
\end{align*}
next iteration,
\begin{align*}
    \bm{r}_1 &= \bm{r}_0 - \alpha_0 \bm{A} \bm{r}_0 = \begin{bmatrix}
        2 \\
        1
    \end{bmatrix} \\
    \beta_0 &= \frac{\bm{r}_1^T \bm{r}_1}{\bm{r}_0^T \bm{r}_0} = \frac{5}{20} = \frac{1}{4} \\
    \bm{p}_1 &= \bm{r}_1 + \beta_0\bm{p}_0 = \begin{bmatrix}
        \frac{5}{2} \\
        0
    \end{bmatrix}\ \\
    \alpha_1 &= \frac{\bm{r}_1^T\bm{r}_1}{\bm{p}_1^T \bm{A} \bm{p}_1} = \frac{2}{5} \\
    \bm{x}_2 &= \bm{x}_1 + \alpha_1 \bm{p}_1 = \begin{bmatrix}
        \frac{12}{7} \\
        -\frac{10}{7}
    \end{bmatrix}
\end{align*}



\section*{Question3}
Let \(f: \R^n \to \R\) be a twice continuously differentiable functions. Suppose that for every \(\bm{x} \in \R^n\), the eigenvalues of the Hessian matrix \(\nabla^2 f(\bm{X})\) lie uniformly in the interval \([\textit{m, M}]\) with \(0 < m \leq M < \infty\).

Prove that:

(a) The function \(f\) has a unique global minimizer \(\bm{x}^{*}\).

(b) For all \(\bm{x} \in \R^n\), the following inequality holds:
\begin{align*}
    \frac{1}{2M} \|\nabla f(x)\|^2 \leq f(\bm{x}) - f(\bm{x}^*) \leq \frac{1}{2m} \|\nabla f(\bm{x})\|^2
\end{align*}

\section*{Answer}
\subsection*{(a)}
We know the second order sufficient condition:
\begin{align*}
    \nabla^2 f(\bm{x}) \succ 0 \implies \text{ \(f\) strictly convex}
\end{align*}
Since all eigenvalues of \(\nabla^2 f(\bm{x}) \geq m > 0\), we know
\begin{align*}
    &\bm{v} \nabla^2 f(\bm{x}) \bm{v} \geq m\|\bm{v}\|^2 > 0 \text{ for any non-zero vector \(\bm{v}\)}  \\
    &\nabla^2 f(\bm{x}) \succ 0
\end{align*} 
Therefore, \(f\) is strictly convex.
And we know the Theorem that for an optimization problem, where \(f: \R^n \to \R\) is strictly convex on \(\Omega\) and \(\Omega\) is a convex set. Then the optimal solution must be unique.

To show \(f\) is coercive, we know
\begin{align*}
    &f(\bm{y}) = f(\bm{x}) + \nabla f(\bm{x})^T (\bm{y} - \bm{x}) + \frac{1}{2} (\bm{y} - \bm{x})^T \nabla^2 f(\bm{z}) (\bm{y} - \bm{x}) \\
    &(\bm{y} - \bm{x})^T \nabla^2 f(\bm{z}) (\bm{y} - \bm{x}) \geq m \|\bm{y} - \bm{x}\|^2 \\
    &f(\bm{y}) \geq f(\bm{x}) + \nabla f(\bm{x})^T (\bm{y} - \bm{x}) + \frac{1}{2} m \|\bm{y} - \bm{x}\|^2  \\
    &f(\bm{y}) \geq f(\bm{0}) + \nabla f(\bm{0})^T \bm{y} + \frac{m}{2} \|\bm{y}\|^2
\end{align*}
Therefore, as \(\|\bm{y}\| \to \infty\), \(f(\bm{y}) \to \infty\). Hence, \(f\) is coercive.
In conclusion, there exists a optimal solution(coercive), and it's unique(strictly convex).

\subsection*{(b)}
We know
\begin{align*}
    &f(\bm{y}) \geq f(\bm{x}) + \nabla f(\bm{x})^T (\bm{y} - \bm{x}) + \frac{1}{2} m \|\bm{y} - \bm{x}\|^2  \\
    &f(\bm{x}^*) \geq f(\bm{x}) + \nabla f(\bm{x})^T (\bm{x}^* - \bm{x}) + \frac{1}{2} m \|\bm{x}^* - \bm{x}\|^2  \\
    &f(\bm{x}) - f(\bm{x}^*) \leq  \nabla f(\bm{x})^T (\bm{x} - \bm{x}^*) - \frac{1}{2} m \|\bm{x} - \bm{x}^*\|^2  \\
\end{align*}
According to Cauchy-Schwarz inequality, we know:
\begin{align*}
    \nabla f(\bm{x})^T \|\bm{x} - \bm{x}^*\| &\leq \|\nabla f(\bm{x})\| \|\bm{x} - \bm{x}^*\|  \\
    f(\bm{x}) - f(\bm{x}^*) &\leq  \|\nabla f(\bm{x})\| \|\bm{x} - \bm{x}^*\| - \frac{1}{2} m \|\bm{x} - \bm{x}^*\|^2  \\
\end{align*}
We know if \(\|\bm{x} - \bm{x}^*\| = \frac{1}{m} \|\nabla f(\bm{x})\|\), we get the maximum.
\begin{align*}
    f(\bm{x}) - f(\bm{x}^*) &\leq  \|\nabla f(\bm{x})\| \frac{1}{m} \|\nabla f(\bm{x})\| - \frac{1}{2} m (\frac{1}{m} \|\nabla f(\bm{x})\|)^2 = \frac{1}{2m} \|\nabla f(\bm{x})\|^2 \\
\end{align*}
Therefore, \( f(\bm{x}) - f(\bm{x}^*) \leq \frac{1}{2m} \|\nabla f(\bm{x})\|^2 \)

And we know:
\begin{align*}
    &f(\bm{y}) \leq f(\bm{x}) + \nabla f(\bm{x})^T (\bm{y} - \bm{x}) + \frac{M}{2} \|\bm{y} - \bm{x}\|^2 \\
    & \bm{y} =  \bm{x} - \frac{1}{M} \nabla f(\bm{x}) \\
    & f(\bm{x} - \frac{1}{M} \nabla f(\bm{x})) \leq f(\bm{x}) + \nabla f(\bm{x})^T (- \frac{1}{M} \nabla f(\bm{x})) + \frac{M}{2} \|\frac{1}{M} \nabla f(\bm{x})\|^2 \\
    & f(\bm{x}) + \nabla f(\bm{x})^T (- \frac{1}{M} \nabla f(\bm{x})) + \frac{M}{2} \|\frac{1}{M} \nabla f(\bm{x})\|^2 = f(\bm{x}) - \frac{1}{2M}\|\nabla f(\bm{x})\|^2 \\
    & f(\bm{x}^*) \leq f(\bm{x} - \frac{1}{M} \nabla f(\bm{x})) \leq f(\bm{x}) - \frac{1}{2M}\|\nabla f(\bm{x})\|^2
\end{align*}

Therefore \(f(\bm{x}) - f(\bm{x}^*) \geq \frac{1}{2M}\|\nabla f(\bm{x})\|^2\)

\section*{Question4}
Consider the optimization problem \(\text{min}_{\bm{x} \in \R^n} f(\bm{x})\), where \(f: \R^n \to \R\) is a continuously differentiable function. To develop a weighted gradient descent method, let \(\bm{W} \in \R^{n \times n}\) be a symmetric positive definite (SPD) matrix. Denote by \(\bm{W}^{\frac{1}{2}}\) the unique SPD square root of \(\bm{W}\) (i.e., \((\bm{W}^{\frac{1}{2}})^2 = \bm{W}\)) and by \(\bm{W}^{-\frac{1}{2}}\) its inverse. Given the current iterate \(\bm{x}^{(k)}\), define the next iterate \(\bm{x}^{(k + 1)}\) as the solution of the following constrained optimization problem:
\begin{align*}
    \text{min}_{\bm{x} \in \R^n}f(\bm{x}^{(k)}) + \langle \nabla f(\bm{x}^{(k)}), \bm{x} - \bm{x}^{(k)}\rangle \\
    \text{subject to } \|\bm{W}^{\frac{1}{2}} (\bm{x} - \bm{x}^(k))\|_2 \leq \alpha_k\|\bm{W}^{-\frac{1}{2}} \nabla f(\bm{x}^(k))\|_2
\end{align*}
where  \(\alpha_k > 0\) is a step-size parameter.

Answer the following questions:

(a) Derive an explicit formula for \(\bm{x}^{(k + 1)}\)

(b) Prove that \(\bm{x}^{(k + 1)}\) is equivalently the unique minimizer of the unconstrained quadratic problem:
\begin{align*}
    \text{min}_{\bm{x} \in \R^n} \left \{ \nabla f(\bm{x}^{(k)}) + \langle f(\bm{x}^{(k)}), \bm{x} - \bm{x}^{(k)} \rangle + \frac{1}{2\alpha_k} \|\bm{W}^{\frac{1}{2}} (\bm{x} - \bm{x}^{(k)})\|_2^2 \right \}
\end{align*}


\end{document}